import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from sklearn import datasets

# 1. Завантаження датасету
data = datasets.fetch_openml(data_id=1499, as_frame=True)
X, y = data.data, data.target

# Перетворення цільового вектору в числові мітки
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Стандартизація ознак
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Розділення на тренувальну і тестову вибірки
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

# Перетворення міток у one-hot encoding
y_train_categorical = to_categorical(y_train)
y_test_categorical = to_categorical(y_test)

# 2. Створення базової моделі
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(y_train_categorical.shape[1], activation='softmax')  # Кількість класів
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# 3. Навчання моделі (10 епох)
history = model.fit(X_train, y_train_categorical, epochs=10, validation_split=0.2, batch_size=16, verbose=1)

# Побудова кривої втрат
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.legend()
plt.show()

# 4. Прогнозування
y_pred = np.argmax(model.predict(X_test), axis=-1)

# 5. Оцінка якості моделі
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

conf_matrix = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(conf_matrix, display_labels=label_encoder.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.show()

# 6. Експерименти з мережею

# a) Більше нейронів та шарів
model_exp1 = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(y_train_categorical.shape[1], activation='softmax')
])

model_exp1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_exp1 = model_exp1.fit(X_train, y_train_categorical, epochs=10, validation_split=0.2, batch_size=16, verbose=1)

# b) Зміна функцій активації
model_exp2 = Sequential([
    Dense(64, activation='tanh', input_shape=(X_train.shape[1],)),
    Dense(32, activation='tanh'),
    Dense(y_train_categorical.shape[1], activation='softmax')
])

model_exp2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_exp2 = model_exp2.fit(X_train, y_train_categorical, epochs=10, validation_split=0.2, batch_size=16, verbose=1)

# c) Інший алгоритм оптимізації (SGD)
model_exp3 = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(y_train_categorical.shape[1], activation='softmax')
])

model_exp3.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])
history_exp3 = model_exp3.fit(X_train, y_train_categorical, epochs=10, validation_split=0.2, batch_size=16, verbose=1)

# Висновок порівняння
plt.plot(history.history['val_loss'], label='Baseline Model')
plt.plot(history_exp1.history['val_loss'], label='More Neurons/Layers')
plt.plot(history_exp2.history['val_loss'], label='Tanh Activation')
plt.plot(history_exp3.history['val_loss'], label='SGD Optimizer')
plt.xlabel('Epochs')
plt.ylabel('Validation Loss')
plt.title('Comparison of Model Variants')
plt.legend()
plt.show()
